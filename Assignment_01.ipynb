{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a01.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kankeinai/physics_classes/blob/main/Assignment_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysA45nwH37Y0"
      },
      "source": [
        "# **PHYS270 Assigmment 1: Lists and Numpy Arrays**\n",
        "Student Name: Begantsova Milana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlc-BZnFrKKA"
      },
      "source": [
        "---\n",
        "\n",
        "## Abstract\n",
        "This experiment tests time efficiency of two popular data structures: numpy arrays and lists. This is done by randomly generating two lists with thousand elements. And by comparison of time needed to lists and numpy arrays to generate a third list/array based on first two. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vCYm3aPsA-i"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Introduction\n",
        "Today we have bunches of data and computiong power to bring to life machine learning and deep learning approaches to produce sufficient predictions or even construct narrow artificial inteligence models. But this analysis is impossible without effective data structures to store our data and manipulate it. Usually all predictions are based on the operations with matrices. In python usually to store matrices we use lists, but there is one more data structure to store matrices: numpy arrays.\n",
        "The goal of this experiment is to analyse what data structure is more effective in terms of time efficiency. When we work with complex neural networks with many hidden layers and data with billions records and features, time is one of the main trouble. So, from the begining we should understand lists or numpy arrays are more effective in computing.\n",
        "Our experiment will consist of three parts. At first we will generate two lists with random numbers and 1000 elements. Later on we will create a new list using values from those lists and substituting them to defined formula. In the last section, we will repeat the same procedure, but with numpy arrays. The time needed to generate third list/numpy array will be compared and I will propose possible reasons for the final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fahCUttEsEJ6"
      },
      "source": [
        "---\n",
        "##Method\n",
        "Two lists will be generated randomly using function random() that returns random floating numbers in interval (-1;1). Two numpy arrays will be based upon previously created lists, so that numpy arrays and lists will operate in the same conditions (with the same numbers), so the chance of \"luck\" will be eliminated.\n",
        "I could use function power to calculate a \"root\" of mathematical expression, but,  in our case, function ***sqrt()*** calculated everything faster. To generate array in Part (c), I used functions from numpy array library, which are known for their effectiveness since vectorization approach in computing. Unfortunetely, lists does not have any built-in functions to compute the needed formula, so standart list generators were used.\n",
        "The main goal of this experiment is to measure what data structure is faster while working with data. There are many possible ready-to-use solutions to measure time execution of the alghoritm. Python has function timeit() from library timeit that accepts number of repetition and function to test and returns average time execution. To use library timeit, we should install it at first with pip. Library time also has function named time.perf_counter() to measure time execution, but the result of this function is not accurate. However, we do not need to install it. As we do not work with functions (only generating lists) and want to install additional libraries, the creators added built-in commands to jupiter notebook to measure time. Among these commands are called %time, %%time, %%timeit. %time and %%time provides only basic time functionality, while %%timeit work as analagous timeit.timeit() function: it executes the same code block over and over again and provides averaged result. In this experiment %%timeit function is the most suitable and accurate solution to measure time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE0-vVMfsfoW"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Experiment\n",
        "At first we should import all neccessary libraries needed while the experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_m53hydwivG"
      },
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "from math import sqrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1afVw_uf74QS"
      },
      "source": [
        "The experiment consists of three parts. Shortly, in the Part (a) we will generate two lists with different 1000 random float numbers. These two lists will be generated using function random( ). Then in Part (b), we should measure time needed to generate a different list elements of which will be calculated according to defined formula using elements from list a and b. In the last Part (c), we will do the same but instead of lists, numpy arrays will be used. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7i8GEwzsT4W"
      },
      "source": [
        "###Part (a)\n",
        "In the first part, we should randomly create two python lists. Lets call them a and b. They should be of the $size=1000$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJWqZ85qyc9l"
      },
      "source": [
        "#The number of elements in the lists\n",
        "size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA8jNcDZu7lR"
      },
      "source": [
        "All of the elements will be generated with function random(  ). To generate those two lists, I will use python shorthand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQe087UIyNtW"
      },
      "source": [
        "#Generate two lists with 1000 elements randomly\n",
        "list_a = [random() for i in range(size)]\n",
        "list_b = [random() for i in range(size)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFx2OE_vsXdp"
      },
      "source": [
        "###Part (b)\n",
        "In this part, we should generate **list c**. The elements of this list should be calculated according to the next formula:\n",
        "$$c[i]=\\sqrt{a[i]^2+b[i]^2}$$\n",
        "So, we should elementwise square elements of **a** and **b**, then sum then one by one, and end up calculating their square root.\n",
        "\n",
        "As it has already been discussed in the method section, I will use jupiter notebook built-in function **%%timeit** to measure time needed to generate **list c**. \n",
        "\n",
        "To generate list c, I will again use python shorthand for creating lists, but this time we work with elements of both lists. Python bult-in lists does not support adding two lists elementwise, so I will use function **zip( )** to create list of tuples (pairs) of elements **a** and **b**. Then in the same manner as in Part (a), I use needed formula to calculate elements of **list c**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gds9YmVcyr2l",
        "outputId": "7e08e222-9b8f-436b-a399-3ada3274f2cb"
      },
      "source": [
        "%%time\n",
        "#Generate list_c using elements of list_a and list_b according to the formula\n",
        "list_c = [sqrt(item_a**2 + item_b**2) for item_a, item_b in zip(list_a, list_b)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 504 µs, sys: 0 ns, total: 504 µs\n",
            "Wall time: 514 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItPSqdAayFSS"
      },
      "source": [
        "As you can see, it took $200 \\mu s$ to generate list of 1000 elements according to formula (1). It is not bad. But \n",
        "in data science, experts work with billions of recording with many features (in this experiment we have only two features: element from list **a** and element from list **b**), so this is quite frustrating result. We generate this list using for-loop, so the time needed to acomplish the task will be linearly proportional to number of elements in a list. On average, the access to each element of a list takes much time due to lists specifics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v57Vs-yJscP-"
      },
      "source": [
        "###Part (c)\n",
        "In this section we will do the same task as in Part (a) and (b), but instead of using lists, we will use numpy arrays.\n",
        "\n",
        "In data science, we usually work with numbers. So there is no need in such generic data structers as lists. They not only have long time of accessing elements, but also don't have efficient implementation of all functions that we need in computing such as element-wise additon, power, substraction and etc. \n",
        "\n",
        "So, there is an alternative: **numpy arrays**. \n",
        "\n",
        "They are created in a such way to be able to make all kind of operations that we can do with matrices. Numpy arrays are based on C-style arrays, which are more low level and fast.\n",
        "\n",
        "Our goal, in this experiment, is to conclude whatever they are more efficient then python lists.\n",
        "\n",
        "I will not generate elements for the second time, as numpy library has built-in function of transforming lists to arrays. If we compare how two data structures work with **the same elements**, the accuracy of the experiment should be highier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dxcTminy3B6"
      },
      "source": [
        "#Tranform lists a and b to numpy arrays\n",
        "array_a = np.array(list_a)\n",
        "array_b = np.array(list_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaSRrQ4F3dl9"
      },
      "source": [
        "As I have already said, when we work with numpy arrays there are ready-to-use functions for finding power, addition and square root extraction. So, there is no need to use a for loop to access elements one by one. We work with matrices instead and vectorized functions instead. To function work properly, arrays should be of the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBDNzVgr1F-M",
        "outputId": "5a3e7b78-5921-4456-971b-c57ad2920e8e"
      },
      "source": [
        "%%timeit\n",
        "#Generate array_c using elements of array_a and array_b according to the formula\n",
        "array_c = np.sqrt(np.add(np.power(array_a, 2), np.power(array_b, 2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 loops, best of 3: 24.4 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOz-qSb14IiT"
      },
      "source": [
        "As you can see, it took $24.4 \\mu s$ to generate an **array c**, which is almost 9 times faster then with lists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3z8sB_Z7SKR"
      },
      "source": [
        "###Summary:\n",
        "Table 1 – Time needed to generate a list/array c\n",
        "\n",
        "Python lists | Numpy arrays\n",
        "--- | ---\n",
        "$200 \\mu s$ | $24.4 \\mu s$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9vk1Z9xstnh"
      },
      "source": [
        "---\n",
        "\n",
        "##Conclusion\n",
        "In this experiment, we generated two sets of random float numbers with 1000 elements to analyse what data structure is more efficient to use while computing. As, it can be seen from the Table 1, numpy arrays are almost 9 times faster than standart python list. The possible reasons for this have been discussed while experiment description. But in short, the python lists are too generic: they can store text, booleans, numbers and etc. As a result, they don't have any built-in functions specified for computing. In addition because of the python lists specifics, lists have quite long time of accessing the elements. In contrast numpy arrays are based on C/C++ language arrays which are low-level, which mean faster accessing data from memory. Numpy library also have bunch of efficient alghorithms created specifically for computing and operations with matrices. Numpy array also support parallel processing, so many operations can completed at the same time. So, numpy arrays are definetely recommended to use in scientific compuatation. In the future, I would like to recommend to conduct an experiment with different number of elements: 10, 50, 100, 500, 1000 and so on. This will help to clearly see how time execution depends on number of elements and chosen data structure. A student can also plot a graph of number of elements versus time execution to comment on these dependencies.\n",
        "After this experiment, I will definetely will use numpy array while working with bunches of data in matrice form."
      ]
    }
  ]
}